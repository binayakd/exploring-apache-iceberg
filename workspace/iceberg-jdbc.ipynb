{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d42eaf5-8ebd-4adc-bd33-a5f58bc74321",
   "metadata": {},
   "source": [
    "# Iceberg with Spark and JDBC Catalog\n",
    "Here we are using Iceberg with Spark, and using JDBC Catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a797deb-91ad-44e2-babb-2d0de7a97947",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\n",
    "Importing the same libraries again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4694ade5-e94c-42aa-97b3-ddd4f9bd183e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from timeit import default_timer as timer\n",
    "import os\n",
    "\n",
    "# this is to better display pyspark dataframes\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5aa829-a51c-4458-86a8-8c494ebaebc5",
   "metadata": {},
   "source": [
    "## Setting up Spark Session\n",
    "We set up Spark Session with with the configs required to connect to the JDBC Catalog. Here we are setting up `iceberg_jdbc` as the iceberg catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf47669-1be8-4008-89f8-99ccecb4b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "iceberg_catalog_name = \"iceberg_jdbc\"\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"iceberg-jdbc\") \\\n",
    "  .config(\"spark.driver.memory\", \"4g\") \\\n",
    "  .config(\"spark.executor.memory\", \"4g\") \\\n",
    "  .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "  .config(\"spark.jars\", \"/opt/extra-jars/iceberg-spark-runtime.jar,/opt/extra-jars/iceberg-aws-bundle.jar,/opt/extra-jars/postgresql.jar\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.type\", \"jdbc\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.uri\", \"jdbc:postgresql://postgres:5432/postgres?currentSchema=iceberg\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.jdbc.user\", \"postgres\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.jdbc.password\", \"postgres\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.jdbc.schema-version\", \"V1\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.warehouse\", \"s3://warehouse/iceberg/\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.s3.endpoint\", \"http://minio:9000\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.s3.path-style-access\", \"true\") \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ea4f2-ed62-4621-9228-9c3777cff826",
   "metadata": {},
   "source": [
    "## Setting up the test parquet file as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "620b1cde-33d2-4355-b3ce-1b14e0e4f039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cc_main_df = spark.read.parquet(\"file:///home/iceberg/workspace/downloaded-data/cc-main-2024-06-26-warc-part-00000.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26486763-4887-4458-a31d-31f369d750b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/02 07:14:03 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+------------+----------------------+----------------------+----------------------+----------------------+------------------------+--------------------------+-----------------------+-----------------------+----------------------+------------+--------+--------------------+---------+-------------------+------------+--------------+--------------------+-----------------+---------------------+---------------+-----------------+-----------------+--------------------+------------------+------------------+----------------+\n",
      "|         url_surtkey|                 url|    url_host_name|url_host_tld|url_host_2nd_last_part|url_host_3rd_last_part|url_host_4th_last_part|url_host_5th_last_part|url_host_registry_suffix|url_host_registered_domain|url_host_private_suffix|url_host_private_domain|url_host_name_reversed|url_protocol|url_port|            url_path|url_query|         fetch_time|fetch_status|fetch_redirect|      content_digest|content_mime_type|content_mime_detected|content_charset|content_languages|content_truncated|       warc_filename|warc_record_offset|warc_record_length|    warc_segment|\n",
      "+--------------------+--------------------+-----------------+------------+----------------------+----------------------+----------------------+----------------------+------------------------+--------------------------+-----------------------+-----------------------+----------------------+------------+--------+--------------------+---------+-------------------+------------+--------------+--------------------+-----------------+---------------------+---------------+-----------------+-----------------+--------------------+------------------+------------------+----------------+\n",
      "|ru,vsepostavshiki...|https://vsepostav...|vsepostavshiki.ru|          ru|        vsepostavshiki|                  NULL|                  NULL|                  NULL|                      ru|         vsepostavshiki.ru|                     ru|      vsepostavshiki.ru|     ru.vsepostavshiki|       https|    NULL|/syre/toplivo-ene...|     NULL|2024-06-25 19:29:32|         200|          NULL|NVDY7QVQSDWZAOVW4...|        text/html|            text/html|          UTF-8|          rus,bak|             NULL|crawl-data/CC-MAI...|         522244472|             12096|1718198866218.13|\n",
      "|ru,vsepostavshiki...|https://vsepostav...|vsepostavshiki.ru|          ru|        vsepostavshiki|                  NULL|                  NULL|                  NULL|                      ru|         vsepostavshiki.ru|                     ru|      vsepostavshiki.ru|     ru.vsepostavshiki|       https|    NULL|     /syre/volgograd|     NULL|2024-06-16 03:23:52|         200|          NULL|FQHYEMDPB5C3DRJOS...|        text/html|            text/html|          UTF-8|              rus|             NULL|crawl-data/CC-MAI...|         567436752|              7849|1718198861640.68|\n",
      "|ru,vsepostavshiki...|https://vsepostav...|vsepostavshiki.ru|          ru|        vsepostavshiki|                  NULL|                  NULL|                  NULL|                      ru|         vsepostavshiki.ru|                     ru|      vsepostavshiki.ru|     ru.vsepostavshiki|       https|    NULL|/syre/vtorichnoe-...|     NULL|2024-06-18 08:55:56|         200|          NULL|2O4O53RXABBX73APU...|        text/html|            text/html|          UTF-8|              rus|             NULL|crawl-data/CC-MAI...|         545647258|             11021|1718198861747.70|\n",
      "|ru,vsepostavshiki...|https://vsepostav...|vsepostavshiki.ru|          ru|        vsepostavshiki|                  NULL|                  NULL|                  NULL|                      ru|         vsepostavshiki.ru|                     ru|      vsepostavshiki.ru|     ru.vsepostavshiki|       https|    NULL|              /tovar|     NULL|2024-06-18 21:15:26|         200|          NULL|QAZLX7HLSJX5C3OAV...|        text/html|            text/html|          UTF-8|              rus|             NULL|crawl-data/CC-MAI...|         537377511|              9228|1718198861794.76|\n",
      "|ru,vsepostavshiki...|https://vsepostav...|vsepostavshiki.ru|          ru|        vsepostavshiki|                  NULL|                  NULL|                  NULL|                      ru|         vsepostavshiki.ru|                     ru|      vsepostavshiki.ru|     ru.vsepostavshiki|       https|    NULL|/tovar/5062-kover...|     NULL|2024-06-20 11:51:07|         200|          NULL|RLVHB6MVQNNXSTPNK...|        text/html|            text/html|          UTF-8|              rus|             NULL|crawl-data/CC-MAI...|         539769264|              9152|1718198861940.83|\n",
      "+--------------------+--------------------+-----------------+------------+----------------------+----------------------+----------------------+----------------------+------------------------+--------------------------+-----------------------+-----------------------+----------------------+------------+--------+--------------------+---------+-------------------+------------+--------------+--------------------+-----------------+---------------------+---------------+-----------------+-----------------+--------------------+------------------+------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cc_main_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c029de8-7e71-4254-b606-959afdfb37ab",
   "metadata": {},
   "source": [
    "## Creating Iceberg namespace under the catalog\n",
    "We create the namespace (schema) under the iceberg catalog `iceberg_jdbc` we created in the Spark Session configs, to demarcate the date being saved using the Rest Java catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c13e29cf-eda1-495c-80fc-a8a98d9519c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/02 07:14:08 WARN JdbcCatalog: JDBC catalog is initialized without view support. To auto-migrate the database's schema and enable view support, set jdbc.schema-version=V1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create namespace iceberg_jdbc.jdbc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a73627-bea3-4676-8abd-02a891cb1824",
   "metadata": {},
   "source": [
    "## Writing the data to Iceberg Table\n",
    "Finally, writing the data to the Iceberg table and timing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8a7b0dd-2c7e-49b2-b722-91c0186c385d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.81681328100967\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "cc_main_df.writeTo(\"iceberg_jdbc.jdbc.cc_main_2024_26_part_00000\").create()\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
