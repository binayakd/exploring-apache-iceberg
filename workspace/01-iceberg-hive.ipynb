{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad8c94fa-f2cd-4e91-884f-d2d598a8b9de",
   "metadata": {},
   "source": [
    "# Hive Catalog\n",
    "So first we test out the Hive Catalog, which is basically using the Hive Metastore as the Iceberg catalog. The Dockerfile and the configurations used to setup the Hive metastore can be found in th `hive-metastore` folder and the Docker Compose file in the repo. This Hive Metastore also connecting to the `hive` database in the postgres instance we have setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c6aecf-28b9-4a5d-947d-53e25ff777ba",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\n",
    "We will be importing `SparkSession` for, well, the Spark session. We also import the Postgress driver `psycopg`, Trino connection libraries, and pandas, to explore the data that we will be writing with Spark.\n",
    "\n",
    "We also set some styling to display tables better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e024242-220a-48dc-bd76-3dde9bd682df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import psycopg\n",
    "from trino.dbapi import connect\n",
    "import pandas as pd\n",
    "\n",
    "# this is to better display pyspark dataframes\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01740cf8-b05c-4016-8808-7b884b10c6c2",
   "metadata": {},
   "source": [
    "## Setting up Spark Session\n",
    "We set up Spark Session with the configs required to connect to the Hive Metastore. \n",
    "\n",
    "It is a single node local spark sessions, setting the driver and executor memories to 4GB, to provide it sufficient memory to load all of the data.\n",
    "We are setting up `iceberg` as the iceberg catalog, and setting up all the required configs to connect to the Hive Metastore catalog ([details here](https://iceberg.apache.org/docs/latest/configuration/#catalog-properties)).\n",
    "\n",
    "To connect to our local instance of Minio, we need to set `s3.endpoint` and `s3.path-style-access` configs, and set our warehouse location to be in the folder `iceberg-hive` under the bucket `warehouse` that was created on startup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ad84ef7-2afb-4bf9-9cfa-8289e2de8c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/09 15:36:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "iceberg_catalog_name = \"iceberg\"\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"iceberg-hive\") \\\n",
    "  .config(\"spark.driver.memory\", \"4g\") \\\n",
    "  .config(\"spark.executor.memory\", \"4g\") \\\n",
    "  .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "  .config(\"spark.jars\", \"/opt/extra-jars/iceberg-spark-runtime.jar,/opt/extra-jars/iceberg-aws-bundle.jar\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.type\", \"hive\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.uri\", \"thrift://hive-metastore:9083\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.warehouse\", \"s3://warehouse/iceberg/\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.s3.endpoint\", \"http://minio:9000\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.s3.path-style-access\", \"true\") \\\n",
    "  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eb09fe-f3b1-44b1-89d2-a0d63861a485",
   "metadata": {},
   "source": [
    "## Loading Test Data\n",
    "Now we load the 2 parquet files downloaded previously, into the Spark memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fb156c4-1b40-47d9-a065-3ea2edb60a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_2024_01 = spark.read.parquet(\"file:///home/iceberg/workspace/downloaded-data/yellow_tripdata_2024-01.parquet\")\n",
    "df_2024_02 = spark.read.parquet(\"file:///home/iceberg/workspace/downloaded-data/yellow_tripdata_2024-02.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b2fde4-1e24-4d61-a684-f1d141c6804e",
   "metadata": {},
   "source": [
    "Now we check the data to get an idea of the size, structure and the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02404d7c-d0f3-40e3-8566-309b21876cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: yellow_tripdata_2024-01.parquet\n",
      "Number of rows: 2964624\n",
      "Schema:\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n",
      "Data:\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-01-01 00:57:55|  2024-01-01 01:17:43|              1|         1.72|         1|                 N|         186|          79|           2|       17.7|  1.0|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              1|          1.8|         1|                 N|         140|         236|           1|       10.0|  3.5|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:17:06|  2024-01-01 00:35:01|              1|          4.7|         1|                 N|         236|          79|           1|       23.3|  3.5|    0.5|       3.0|         0.0|                  1.0|        31.3|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              1|          1.4|         1|                 N|          79|         211|           1|       10.0|  3.5|    0.5|       2.0|         0.0|                  1.0|        17.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              1|          0.8|         1|                 N|         211|         148|           1|        7.9|  3.5|    0.5|       3.2|         0.0|                  1.0|        16.1|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"file: yellow_tripdata_2024-01.parquet\")\n",
    "print(f\"Number of rows: {df_2024_01.count()}\")\n",
    "print(\"Schema:\")\n",
    "df_2024_01.printSchema()\n",
    "print(\"Data:\")\n",
    "df_2024_01.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76d1e9ea-8ef2-4e79-ad1d-a68857eb4d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: yellow_tripdata_2024-02.parquet\n",
      "Number of rows: 3007526\n",
      "Schema:\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n",
      "Data:\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-02-01 00:04:45|  2024-02-01 00:19:58|              1|         4.39|         1|                 N|          68|         236|           1|       20.5|  1.0|    0.5|      1.28|         0.0|                  1.0|       26.78|                 2.5|        0.0|\n",
      "|       2| 2024-02-01 00:56:31|  2024-02-01 01:10:53|              1|         7.71|         1|                 N|          48|         243|           1|       31.0|  1.0|    0.5|       9.0|         0.0|                  1.0|        45.0|                 2.5|        0.0|\n",
      "|       2| 2024-02-01 00:07:50|  2024-02-01 00:43:12|              2|        28.69|         2|                 N|         132|         261|           2|       70.0|  0.0|    0.5|       0.0|        6.94|                  1.0|       82.69|                 2.5|       1.75|\n",
      "|       1| 2024-02-01 00:01:49|  2024-02-01 00:10:47|              1|          1.1|         1|                 N|         161|         163|           1|        9.3|  3.5|    0.5|      2.85|         0.0|                  1.0|       17.15|                 2.5|        0.0|\n",
      "|       1| 2024-02-01 00:37:35|  2024-02-01 00:51:15|              1|          2.6|         1|                 N|         246|          79|           2|       15.6|  3.5|    0.5|       0.0|         0.0|                  1.0|        20.6|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/09 15:36:27 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "print(\"file: yellow_tripdata_2024-02.parquet\")\n",
    "print(f\"Number of rows: {df_2024_02.count()}\")\n",
    "print(\"Schema:\")\n",
    "df_2024_02.printSchema()\n",
    "print(\"Data:\")\n",
    "df_2024_02.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0028573-a676-4ed9-857e-25538fab0bd8",
   "metadata": {},
   "source": [
    "Data look good, so now on to actually writing it to our Iceberg catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8092ce-a336-48b4-9874-969ae3d8b0f3",
   "metadata": {},
   "source": [
    "## Creating Iceberg namespace under the catalog\n",
    "First, we need to create a new namespace (schema) under the iceberg catalog. Here we create the namespace `hive` under the catalog, and assign a location in Minio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9117aa16-1313-42ae-92e1-5a1dbb2c595f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg.hive LOCATION 's3://warehouse/iceberg/hive'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358475ce-47c2-46e3-9b04-3c72671d092e",
   "metadata": {},
   "source": [
    "## Writing the data to Iceberg Table\n",
    "We want to create this table first, based on 2024-01 data, partitioned by the month. We can get the month from the `tpep_pickup_datetime` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "920e67d3-8e4d-4f25-9d11-b463387f3dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_2024_01.writeTo(\"iceberg.hive.yellow_tripdata\").partitionedBy(\n",
    "    F.months(\"tpep_pickup_datetime\")\n",
    ").create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1487228f-b279-4d9e-bb11-e84a08a7f90f",
   "metadata": {},
   "source": [
    "We then check how the data is saved to Minio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4612d6ed-abb4-4007-9ab4-7adb4a3c3e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]11;?\u001b\\\u001b[6n\u001b[m\u001b[32m[2024-09-09 15:36:46 UTC]\u001b[0m\u001b[33m     0B\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[36;1m iceberg/hive/\u001b[0;22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-09 15:49:23 UTC]\u001b[0m\u001b[33m 5.9KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/data/tpep_pickup_datetime_month=2002-12/00000-38-67af578d-850f-4bd2-8503-844b0f3192ba-0-00003.parquet\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-09 15:49:23 UTC]\u001b[0m\u001b[33m 5.9KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/data/tpep_pickup_datetime_month=2009-01/00000-38-67af578d-850f-4bd2-8503-844b0f3192ba-0-00004.parquet\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-09 15:49:23 UTC]\u001b[0m\u001b[33m 6.3KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/data/tpep_pickup_datetime_month=2023-12/00000-38-67af578d-850f-4bd2-8503-844b0f3192ba-0-00001.parquet\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-09 15:49:23 UTC]\u001b[0m\u001b[33m  44MiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/data/tpep_pickup_datetime_month=2024-01/00000-38-67af578d-850f-4bd2-8503-844b0f3192ba-0-00002.parquet\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-09 15:49:23 UTC]\u001b[0m\u001b[33m 5.9KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/data/tpep_pickup_datetime_month=2024-02/00000-38-67af578d-850f-4bd2-8503-844b0f3192ba-0-00005.parquet\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-09 15:49:23 UTC]\u001b[0m\u001b[33m 3.8KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/metadata/00000-eb2ed37f-f7f9-4bf5-89c0-572f46d18b36.metadata.json\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-09 15:49:23 UTC]\u001b[0m\u001b[33m 9.0KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/metadata/f588d9e8-11ba-4a52-928a-2d002b5b42db-m0.avro\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-09 15:49:23 UTC]\u001b[0m\u001b[33m 4.2KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/metadata/snap-8347670030789304497-1-f588d9e8-11ba-4a52-928a-2d002b5b42db.avro\u001b[22m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!mc ls --recursive minio/warehouse/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9124c2-c732-46c5-8d6c-af7111a327d7",
   "metadata": {},
   "source": [
    "There something interesting here. We are expecting this file to only have data for the month of 2024-01, but there seems to be some data from some other months. Although looking at the size of the partitions, we can see the expected partition month is the biggest, and the rest of the partitions could have some bad data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73fc04d-686e-4e3f-8bb2-7f50336dec35",
   "metadata": {},
   "source": [
    "We also check what metadata has been written to the Hive Metastore's attached database. Using the `psycopg` and `pandas` library, can get the data from specific table that the HIve metastore wrote to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b98f5eec-99b5-462a-8ed1-dddc13c6d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg.connect(\"postgresql://postgres:postgres@postgres:5432/hive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640f8870-8b86-4223-b50f-28ba7b102ecb",
   "metadata": {},
   "source": [
    "The first table is the `DBS` table, which shows that there is a default database with the location pointing to the local file system. This explains why we need to create a new namespace with the location set to our object storage, which is the second row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4d30056-b3cf-417e-83d8-9e417c917bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_724/2880036002.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  pd.read_sql_query('select * from \"DBS\"', conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DB_ID</th>\n",
       "      <th>DESC</th>\n",
       "      <th>DB_LOCATION_URI</th>\n",
       "      <th>NAME</th>\n",
       "      <th>OWNER_NAME</th>\n",
       "      <th>OWNER_TYPE</th>\n",
       "      <th>CTLG_NAME</th>\n",
       "      <th>CREATE_TIME</th>\n",
       "      <th>DB_MANAGED_LOCATION_URI</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>DATACONNECTOR_NAME</th>\n",
       "      <th>REMOTE_DBNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Default Hive database</td>\n",
       "      <td>file:/user/hive/warehouse</td>\n",
       "      <td>default</td>\n",
       "      <td>public</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>hive</td>\n",
       "      <td>1725788794</td>\n",
       "      <td>None</td>\n",
       "      <td>NATIVE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://warehouse/iceberg/hive</td>\n",
       "      <td>hive</td>\n",
       "      <td>iceberg</td>\n",
       "      <td>USER</td>\n",
       "      <td>hive</td>\n",
       "      <td>1725896204</td>\n",
       "      <td>None</td>\n",
       "      <td>NATIVE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DB_ID                   DESC              DB_LOCATION_URI     NAME  \\\n",
       "0      1  Default Hive database    file:/user/hive/warehouse  default   \n",
       "1      2                   None  s3://warehouse/iceberg/hive     hive   \n",
       "\n",
       "  OWNER_NAME OWNER_TYPE CTLG_NAME  CREATE_TIME DB_MANAGED_LOCATION_URI  \\\n",
       "0     public       ROLE      hive   1725788794                    None   \n",
       "1    iceberg       USER      hive   1725896204                    None   \n",
       "\n",
       "     TYPE DATACONNECTOR_NAME REMOTE_DBNAME  \n",
       "0  NATIVE               None          None  \n",
       "1  NATIVE               None          None  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query('select * from \"DBS\"', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c57145-f6b9-4412-915e-50b552169cc7",
   "metadata": {},
   "source": [
    "Next we can look at the `TBLS` table, which shows the record of our recently created Iceberg table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef318e1e-1f6e-4dd4-b542-bb805da16786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_724/1911613634.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  pd.read_sql_query('select * from \"TBLS\"', conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TBL_ID</th>\n",
       "      <th>CREATE_TIME</th>\n",
       "      <th>DB_ID</th>\n",
       "      <th>LAST_ACCESS_TIME</th>\n",
       "      <th>OWNER</th>\n",
       "      <th>OWNER_TYPE</th>\n",
       "      <th>RETENTION</th>\n",
       "      <th>SD_ID</th>\n",
       "      <th>TBL_NAME</th>\n",
       "      <th>TBL_TYPE</th>\n",
       "      <th>VIEW_EXPANDED_TEXT</th>\n",
       "      <th>VIEW_ORIGINAL_TEXT</th>\n",
       "      <th>IS_REWRITE_ENABLED</th>\n",
       "      <th>WRITE_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1725896964</td>\n",
       "      <td>2</td>\n",
       "      <td>-679888</td>\n",
       "      <td>iceberg</td>\n",
       "      <td>USER</td>\n",
       "      <td>2147483647</td>\n",
       "      <td>1</td>\n",
       "      <td>yellow_tripdata</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TBL_ID  CREATE_TIME  DB_ID  LAST_ACCESS_TIME    OWNER OWNER_TYPE  \\\n",
       "0       1   1725896964      2           -679888  iceberg       USER   \n",
       "\n",
       "    RETENTION  SD_ID         TBL_NAME        TBL_TYPE VIEW_EXPANDED_TEXT  \\\n",
       "0  2147483647      1  yellow_tripdata  EXTERNAL_TABLE               None   \n",
       "\n",
       "  VIEW_ORIGINAL_TEXT  IS_REWRITE_ENABLED  WRITE_ID  \n",
       "0               None               False         0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query('select * from \"TBLS\"', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a3c358-8635-4ca1-bcf4-6c1213eee613",
   "metadata": {},
   "source": [
    "Finally we look at the `TABLE_PARAMS` table, which has the more information about the created Iceberg table, such as the table statistics, the current snapshot summary and schema, and the location of the Iceberg table metadata in Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c08c7645-a899-432d-9c9b-bd24917fb466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_724/1897984218.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  pd.read_sql_query('select * from \"TABLE_PARAMS\"', conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TBL_ID</th>\n",
       "      <th>PARAM_KEY</th>\n",
       "      <th>PARAM_VALUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>default-partition-spec</td>\n",
       "      <td>{\"spec-id\":0,\"fields\":[{\"name\":\"tpep_pickup_da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>current-schema</td>\n",
       "      <td>{\"type\":\"struct\",\"schema-id\":0,\"fields\":[{\"id\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>uuid</td>\n",
       "      <td>c5c14cb9-a698-4d6f-b2d8-a03c1f83953b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>transient_lastDdlTime</td>\n",
       "      <td>1725896964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>write.parquet.compression-codec</td>\n",
       "      <td>zstd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>owner</td>\n",
       "      <td>iceberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>table_type</td>\n",
       "      <td>ICEBERG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>numFilesErasureCoded</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>EXTERNAL</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>numRows</td>\n",
       "      <td>5972150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>numFiles</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>previous_metadata_location</td>\n",
       "      <td>s3://warehouse/iceberg/hive/yellow_tripdata/me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>current-snapshot-id</td>\n",
       "      <td>8346019809249799834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>totalSize</td>\n",
       "      <td>93238954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>current-snapshot-timestamp-ms</td>\n",
       "      <td>1725982446355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>metadata_location</td>\n",
       "      <td>s3://warehouse/iceberg/hive/yellow_tripdata/me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>snapshot-count</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>current-snapshot-summary</td>\n",
       "      <td>{\"spark.app.id\":\"local-1725896172891\",\"added-d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TBL_ID                        PARAM_KEY  \\\n",
       "0        1           default-partition-spec   \n",
       "1        1                   current-schema   \n",
       "2        1                             uuid   \n",
       "3        1            transient_lastDdlTime   \n",
       "4        1  write.parquet.compression-codec   \n",
       "5        1                            owner   \n",
       "6        1                       table_type   \n",
       "7        1             numFilesErasureCoded   \n",
       "8        1                         EXTERNAL   \n",
       "9        1                          numRows   \n",
       "10       1                         numFiles   \n",
       "11       1       previous_metadata_location   \n",
       "12       1              current-snapshot-id   \n",
       "13       1                        totalSize   \n",
       "14       1    current-snapshot-timestamp-ms   \n",
       "15       1                metadata_location   \n",
       "16       1                   snapshot-count   \n",
       "17       1         current-snapshot-summary   \n",
       "\n",
       "                                          PARAM_VALUE  \n",
       "0   {\"spec-id\":0,\"fields\":[{\"name\":\"tpep_pickup_da...  \n",
       "1   {\"type\":\"struct\",\"schema-id\":0,\"fields\":[{\"id\"...  \n",
       "2                c5c14cb9-a698-4d6f-b2d8-a03c1f83953b  \n",
       "3                                          1725896964  \n",
       "4                                                zstd  \n",
       "5                                             iceberg  \n",
       "6                                             ICEBERG  \n",
       "7                                                   0  \n",
       "8                                                TRUE  \n",
       "9                                             5972150  \n",
       "10                                                 10  \n",
       "11  s3://warehouse/iceberg/hive/yellow_tripdata/me...  \n",
       "12                                8346019809249799834  \n",
       "13                                           93238954  \n",
       "14                                      1725982446355  \n",
       "15  s3://warehouse/iceberg/hive/yellow_tripdata/me...  \n",
       "16                                                  2  \n",
       "17  {\"spark.app.id\":\"local-1725896172891\",\"added-d...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query('select * from \"TABLE_PARAMS\"', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb9beb9-4dd0-4a4a-9b1b-89c88686d5c6",
   "metadata": {},
   "source": [
    "There are other tables that get updated, but these are the main one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e497e4-0b42-462c-818a-20596d5efda9",
   "metadata": {},
   "source": [
    "## Adding New partition to the table\n",
    "Now, we will add the file for the month of 2024-02 as a new partition to the table we just created. we can do that by using the `append` option of the write command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3ad80cc-7c78-4129-abc3-ff3be3d472b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_2024_02.writeTo(\"iceberg.hive.yellow_tripdata\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695bfccf-cc2a-43ad-a1e2-50049ecb10cc",
   "metadata": {},
   "source": [
    "And we check the data in Minio again, to see if the new partition has been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43ca7b39-c955-43b3-b3fa-a14acadf8195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]11;?\u001b\\\u001b[6n\u001b[m\u001b[32m[2024-09-09 15:36:46 UTC]\u001b[0m\u001b[33m     0B\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[36;1m iceberg/hive/\u001b[0;22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-09 15:49:23 UTC]\u001b[0m\u001b[33m 5.9KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/data/tpep_pickup_datetime_month=2002-12/00000-38-67af578d-850f-4bd2-8503-844b0f3192ba-0-00003.parquet\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-10 15:34:06 UTC]\u001b[0m\u001b[33m 5.3KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/data/tpep_pickup_datetime_month=2008-12/00000-47-81691b34-dddc-4409-ad0f-982b7862cc59-0-00003.parquet\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-09 15:49:23 UTC]\u001b[0m\u001b[33m 5.9KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/data/tpep_pickup_datetime_month=2009-01/00000-38-67af578d-850f-4bd2-8503-844b0f3192ba-0-00004.parquet\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-10 15:34:06 UTC]\u001b[0m\u001b[33m 5.3KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/data/tpep_pickup_datetime_month=2009-01/00000-47-81691b34-dddc-4409-ad0f-982b7862cc59-0-00004.parquet\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-09 15:49:23 UTC]\u001b[0m\u001b[33m 6.3KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/data/tpep_pickup_datetime_month=2023-12/00000-38-67af578d-850f-4bd2-8503-844b0f3192ba-0-00001.parquet\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-09 15:49:23 UTC]\u001b[0m\u001b[33m  44MiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/data/tpep_pickup_datetime_month=2024-01/00000-38-67af578d-850f-4bd2-8503-844b0f3192ba-0-00002.parquet\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-10 15:34:05 UTC]\u001b[0m\u001b[33m 6.3KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/data/tpep_pickup_datetime_month=2024-01/00000-47-81691b34-dddc-4409-ad0f-982b7862cc59-0-00001.parquet\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-09 15:49:23 UTC]\u001b[0m\u001b[33m 5.9KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/data/tpep_pickup_datetime_month=2024-02/00000-38-67af578d-850f-4bd2-8503-844b0f3192ba-0-00005.parquet\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-10 15:34:06 UTC]\u001b[0m\u001b[33m  44MiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/data/tpep_pickup_datetime_month=2024-02/00000-47-81691b34-dddc-4409-ad0f-982b7862cc59-0-00002.parquet\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-10 15:34:06 UTC]\u001b[0m\u001b[33m 5.8KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/data/tpep_pickup_datetime_month=2024-03/00000-47-81691b34-dddc-4409-ad0f-982b7862cc59-0-00005.parquet\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-09 15:49:23 UTC]\u001b[0m\u001b[33m 3.8KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/metadata/00000-eb2ed37f-f7f9-4bf5-89c0-572f46d18b36.metadata.json\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-10 15:34:06 UTC]\u001b[0m\u001b[33m 4.9KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/metadata/00001-9d6e0414-9de9-45d9-8f4f-1f06255d3369.metadata.json\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-10 15:34:06 UTC]\u001b[0m\u001b[33m 8.9KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/metadata/e6f7021b-7822-4c96-9e6d-61f4c42c2e44-m0.avro\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-09 15:49:23 UTC]\u001b[0m\u001b[33m 9.0KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/metadata/f588d9e8-11ba-4a52-928a-2d002b5b42db-m0.avro\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-10 15:34:06 UTC]\u001b[0m\u001b[33m 4.2KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/metadata/snap-8346019809249799834-1-e6f7021b-7822-4c96-9e6d-61f4c42c2e44.avro\u001b[22m\u001b[m\n",
      "\u001b[m\u001b[32m[2024-09-09 15:49:23 UTC]\u001b[0m\u001b[33m 4.2KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m iceberg/hive/yellow_tripdata/metadata/snap-8347670030789304497-1-f588d9e8-11ba-4a52-928a-2d002b5b42db.avro\u001b[22m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!mc ls --recursive minio/warehouse/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49143e2a-e1cd-4839-a9c9-5c0e2fe80647",
   "metadata": {},
   "source": [
    "Again we see the expected partition created, and some extra partitions with stray data. We also see an new setup of metadata files being created. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67592ea6-3263-4891-bb44-3ac38b385565",
   "metadata": {},
   "source": [
    "Querying the snapshots for this table, we can see there are 2, one for the creation of the table, and one for addition of the next partition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d64071d-8b83-4d7a-9ea5-0adc5e8b6498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2024-09-09 15:49:...|8347670030789304497|               NULL|   append|s3://warehouse/ic...|{spark.app.id -> ...|\n",
      "|2024-09-10 15:34:...|8346019809249799834|8347670030789304497|   append|s3://warehouse/ic...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from iceberg.hive.yellow_tripdata.snapshots\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e8696b-6393-447c-9f16-936a2dd67416",
   "metadata": {},
   "source": [
    "We can also run a query to check the stats for all the partitions in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bb3eac0-6184-4263-8e89-1cc68c4fa109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|partition|spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|     last_updated_at|last_updated_snapshot_id|\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|    {648}|      0|     2964617|         2|                     46495595|                           0|                         0|                           0|                         0|2024-09-10 15:34:...|     8346019809249799834|\n",
      "|    {649}|      0|     3007514|         2|                     46708043|                           0|                         0|                           0|                         0|2024-09-10 15:34:...|     8346019809249799834|\n",
      "|    {647}|      0|          10|         1|                         6418|                           0|                         0|                           0|                         0|2024-09-09 15:49:...|     8347670030789304497|\n",
      "|    {650}|      0|           2|         1|                         5908|                           0|                         0|                           0|                         0|2024-09-10 15:34:...|     8346019809249799834|\n",
      "|    {395}|      0|           2|         1|                         6043|                           0|                         0|                           0|                         0|2024-09-09 15:49:...|     8347670030789304497|\n",
      "|    {468}|      0|           4|         2|                        11514|                           0|                         0|                           0|                         0|2024-09-10 15:34:...|     8346019809249799834|\n",
      "|    {467}|      0|           1|         1|                         5433|                           0|                         0|                           0|                         0|2024-09-10 15:34:...|     8346019809249799834|\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from iceberg.hive.yellow_tripdata.partitions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef65b0c-3841-44ed-a7d1-730b60f07b07",
   "metadata": {},
   "source": [
    "## Querying with Trino\n",
    "To start querying the data with Trino, we first need to configure Trino to connect to the [Hive catalog](https://trino.io/docs/current/object-storage/metastores.html#hive-thrift-metastore) using the following catalog properties (which has already been setup in the Trino configuration folder):\n",
    "\n",
    "```\n",
    "connector.name=iceberg\n",
    "iceberg.catalog.type=hive_metastore\n",
    "hive.metastore.uri=thrift://hive-metastore:9083\n",
    "fs.native-s3.enabled=true\n",
    "s3.endpoint=http://minio:9000\n",
    "s3.path-style-access=true\n",
    "s3.aws-access-key=${ENV:AWS_ACCESS_KEY_ID}\n",
    "s3.aws-secret-key=${ENV:AWS_SECRET_ACCESS_KEY}\n",
    "s3.region=${ENV:AWS_REGION}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d8a757-f649-4972-897c-5e3a3e9e8143",
   "metadata": {},
   "source": [
    "We then use the Trino python client, together with pandas to ready the data back. First we setup the connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0189ca29-8c1c-4561-a81c-3864034a0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trino_conn = connect(\n",
    "    host=\"trino\",\n",
    "    port=8080,\n",
    "    user=\"user\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc40126-f404-42c0-a91d-66a1fbab507a",
   "metadata": {},
   "source": [
    "Then we read the data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ed1cc15-7506-4c52-a659-618019b0a7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_724/306683282.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  pd.read_sql_query('select * from \"iceberg-hive\".hive.yellow_tripdata limit 10', trino_conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendorid</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>ratecodeid</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>pulocationid</th>\n",
       "      <th>dolocationid</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-01 00:57:55</td>\n",
       "      <td>2024-01-01 01:17:43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>186</td>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>17.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.70</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 00:36:38</td>\n",
       "      <td>2024-01-01 00:44:56</td>\n",
       "      <td>1</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>79</td>\n",
       "      <td>211</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 00:46:51</td>\n",
       "      <td>2024-01-01 00:52:57</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>211</td>\n",
       "      <td>148</td>\n",
       "      <td>1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.10</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 00:54:08</td>\n",
       "      <td>2024-01-01 01:26:31</td>\n",
       "      <td>1</td>\n",
       "      <td>4.70</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>148</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>29.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-01 00:49:44</td>\n",
       "      <td>2024-01-01 01:15:47</td>\n",
       "      <td>2</td>\n",
       "      <td>10.82</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>138</td>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>45.7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 00:03:00</td>\n",
       "      <td>2024-01-01 00:09:36</td>\n",
       "      <td>1</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>140</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.75</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 00:17:06</td>\n",
       "      <td>2024-01-01 00:35:01</td>\n",
       "      <td>1</td>\n",
       "      <td>4.70</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>236</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>23.3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 00:30:40</td>\n",
       "      <td>2024-01-01 00:58:40</td>\n",
       "      <td>0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>246</td>\n",
       "      <td>231</td>\n",
       "      <td>2</td>\n",
       "      <td>25.4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.40</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-01 00:26:01</td>\n",
       "      <td>2024-01-01 00:54:12</td>\n",
       "      <td>1</td>\n",
       "      <td>5.44</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>161</td>\n",
       "      <td>261</td>\n",
       "      <td>2</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-01 00:28:08</td>\n",
       "      <td>2024-01-01 00:29:16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>113</td>\n",
       "      <td>113</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vendorid tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         2  2024-01-01 00:57:55   2024-01-01 01:17:43                1   \n",
       "1         1  2024-01-01 00:36:38   2024-01-01 00:44:56                1   \n",
       "2         1  2024-01-01 00:46:51   2024-01-01 00:52:57                1   \n",
       "3         1  2024-01-01 00:54:08   2024-01-01 01:26:31                1   \n",
       "4         2  2024-01-01 00:49:44   2024-01-01 01:15:47                2   \n",
       "5         1  2024-01-01 00:03:00   2024-01-01 00:09:36                1   \n",
       "6         1  2024-01-01 00:17:06   2024-01-01 00:35:01                1   \n",
       "7         1  2024-01-01 00:30:40   2024-01-01 00:58:40                0   \n",
       "8         2  2024-01-01 00:26:01   2024-01-01 00:54:12                1   \n",
       "9         2  2024-01-01 00:28:08   2024-01-01 00:29:16                1   \n",
       "\n",
       "   trip_distance  ratecodeid store_and_fwd_flag  pulocationid  dolocationid  \\\n",
       "0           1.72           1                  N           186            79   \n",
       "1           1.40           1                  N            79           211   \n",
       "2           0.80           1                  N           211           148   \n",
       "3           4.70           1                  N           148           141   \n",
       "4          10.82           1                  N           138           181   \n",
       "5           1.80           1                  N           140           236   \n",
       "6           4.70           1                  N           236            79   \n",
       "7           3.00           1                  N           246           231   \n",
       "8           5.44           1                  N           161           261   \n",
       "9           0.04           1                  N           113           113   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             2         17.7    1.0      0.5        0.00           0.0   \n",
       "1             1         10.0    3.5      0.5        2.00           0.0   \n",
       "2             1          7.9    3.5      0.5        3.20           0.0   \n",
       "3             1         29.6    3.5      0.5        6.90           0.0   \n",
       "4             1         45.7    6.0      0.5       10.00           0.0   \n",
       "5             1         10.0    3.5      0.5        3.75           0.0   \n",
       "6             1         23.3    3.5      0.5        3.00           0.0   \n",
       "7             2         25.4    3.5      0.5        0.00           0.0   \n",
       "8             2         31.0    1.0      0.5        0.00           0.0   \n",
       "9             2          3.0    1.0      0.5        0.00           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
       "0                    1.0         22.70                   2.5         0.00  \n",
       "1                    1.0         17.00                   2.5         0.00  \n",
       "2                    1.0         16.10                   2.5         0.00  \n",
       "3                    1.0         41.50                   2.5         0.00  \n",
       "4                    1.0         64.95                   0.0         1.75  \n",
       "5                    1.0         18.75                   2.5         0.00  \n",
       "6                    1.0         31.30                   2.5         0.00  \n",
       "7                    1.0         30.40                   2.5         0.00  \n",
       "8                    1.0         36.00                   2.5         0.00  \n",
       "9                    1.0          8.00                   2.5         0.00  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query('select * from \"iceberg-hive\".hive.yellow_tripdata limit 10', trino_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec304c6f-6c27-4c96-9777-9ddc29e75ba2",
   "metadata": {},
   "source": [
    "We can also use Trino to query the Iceberg metadata, with a slightly different syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aaf70db5-db4a-47a7-9290-94a6c33d6c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_724/2516003307.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  pd.read_sql_query('select * from \"iceberg-hive\".hive.\"yellow_tripdata$snapshots\"', trino_conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>committed_at</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>operation</th>\n",
       "      <th>manifest_list</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-09-09 15:49:23.822000+00:00</td>\n",
       "      <td>8347670030789304497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>append</td>\n",
       "      <td>s3://warehouse/iceberg/hive/yellow_tripdata/me...</td>\n",
       "      <td>{'spark.app.id': 'local-1725896172891', 'chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-10 15:34:06.355000+00:00</td>\n",
       "      <td>8346019809249799834</td>\n",
       "      <td>8.347670e+18</td>\n",
       "      <td>append</td>\n",
       "      <td>s3://warehouse/iceberg/hive/yellow_tripdata/me...</td>\n",
       "      <td>{'spark.app.id': 'local-1725896172891', 'chang...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      committed_at          snapshot_id     parent_id  \\\n",
       "0 2024-09-09 15:49:23.822000+00:00  8347670030789304497           NaN   \n",
       "1 2024-09-10 15:34:06.355000+00:00  8346019809249799834  8.347670e+18   \n",
       "\n",
       "  operation                                      manifest_list  \\\n",
       "0    append  s3://warehouse/iceberg/hive/yellow_tripdata/me...   \n",
       "1    append  s3://warehouse/iceberg/hive/yellow_tripdata/me...   \n",
       "\n",
       "                                             summary  \n",
       "0  {'spark.app.id': 'local-1725896172891', 'chang...  \n",
       "1  {'spark.app.id': 'local-1725896172891', 'chang...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query('select * from \"iceberg-hive\".hive.\"yellow_tripdata$snapshots\"', trino_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63f585ee-a7da-4af2-91af-65bf8ef61fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_724/3259648639.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  pd.read_sql_query('select * from \"iceberg-hive\".hive.\"yellow_tripdata$partitions\"', trino_conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>partition</th>\n",
       "      <th>record_count</th>\n",
       "      <th>file_count</th>\n",
       "      <th>total_size</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(tpep_pickup_datetime_month: 467)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5433</td>\n",
       "      <td>(VendorID: (min: 2, max: 2, null_count: 0, nan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(tpep_pickup_datetime_month: 468)</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>11514</td>\n",
       "      <td>(VendorID: (min: 2, max: 2, null_count: 0, nan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(tpep_pickup_datetime_month: 647)</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>6418</td>\n",
       "      <td>(VendorID: (min: 2, max: 2, null_count: 0, nan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(tpep_pickup_datetime_month: 648)</td>\n",
       "      <td>2964617</td>\n",
       "      <td>2</td>\n",
       "      <td>46495595</td>\n",
       "      <td>(VendorID: (min: 1, max: 6, null_count: 0, nan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(tpep_pickup_datetime_month: 395)</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6043</td>\n",
       "      <td>(VendorID: (min: 2, max: 2, null_count: 0, nan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(tpep_pickup_datetime_month: 649)</td>\n",
       "      <td>3007514</td>\n",
       "      <td>2</td>\n",
       "      <td>46708043</td>\n",
       "      <td>(VendorID: (min: 1, max: 2, null_count: 0, nan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(tpep_pickup_datetime_month: 650)</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5908</td>\n",
       "      <td>(VendorID: (min: 2, max: 2, null_count: 0, nan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           partition  record_count  file_count  total_size  \\\n",
       "0  (tpep_pickup_datetime_month: 467)             1           1        5433   \n",
       "1  (tpep_pickup_datetime_month: 468)             4           2       11514   \n",
       "2  (tpep_pickup_datetime_month: 647)            10           1        6418   \n",
       "3  (tpep_pickup_datetime_month: 648)       2964617           2    46495595   \n",
       "4  (tpep_pickup_datetime_month: 395)             2           1        6043   \n",
       "5  (tpep_pickup_datetime_month: 649)       3007514           2    46708043   \n",
       "6  (tpep_pickup_datetime_month: 650)             2           1        5908   \n",
       "\n",
       "                                                data  \n",
       "0  (VendorID: (min: 2, max: 2, null_count: 0, nan...  \n",
       "1  (VendorID: (min: 2, max: 2, null_count: 0, nan...  \n",
       "2  (VendorID: (min: 2, max: 2, null_count: 0, nan...  \n",
       "3  (VendorID: (min: 1, max: 6, null_count: 0, nan...  \n",
       "4  (VendorID: (min: 2, max: 2, null_count: 0, nan...  \n",
       "5  (VendorID: (min: 1, max: 2, null_count: 0, nan...  \n",
       "6  (VendorID: (min: 2, max: 2, null_count: 0, nan...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query('select * from \"iceberg-hive\".hive.\"yellow_tripdata$partitions\"', trino_conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
