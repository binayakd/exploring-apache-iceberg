{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad8c94fa-f2cd-4e91-884f-d2d598a8b9de",
   "metadata": {},
   "source": [
    "# Iceberg with Spark and Hive Metastore Catalog\n",
    "Here we are using Iceberg with Spark, and using Hive Metastore as the Catalog. This is the OC catalog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c6aecf-28b9-4a5d-947d-53e25ff777ba",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\n",
    "We will be importing `SparkSession` and `os`, which is used to read environment variable for the Minio access key and secret.\n",
    "\n",
    "We also set some styling to display tables better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e024242-220a-48dc-bd76-3dde9bd682df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# this is to better display pyspark dataframes\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01740cf8-b05c-4016-8808-7b884b10c6c2",
   "metadata": {},
   "source": [
    "## Setting up Spark Session\n",
    "We set up Spark Session with with the configs required to connect to the Hive Metastore. Here we are setting up `spark_iceberg_hive` as the iceberg catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad84ef7-2afb-4bf9-9cfa-8289e2de8c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/24 16:35:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/24 16:35:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "iceberg_catalog_name = \"spark_iceberg_hive\"\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"iceberg-hive\") \\\n",
    "  .config(\"spark.driver.memory\", \"4g\") \\\n",
    "  .config(\"spark.executor.memory\", \"4g\") \\\n",
    "  .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "  .config(\"spark.jars\", \"/opt/extra-jars/iceberg-spark-runtime.jar,/opt/extra-jars/iceberg-aws-bundle.jar\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.type\", \"hive\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.uri\", \"thrift://hive-metastore:9083\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.warehouse\", \"s3://warehouse/spark-iceberg-hive/\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.s3.endpoint\", \"http://minio:9000\") \\\n",
    "  .config(f\"spark.sql.catalog.{iceberg_catalog_name}.s3.path-style-access\", \"true\") \\\n",
    "  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eb09fe-f3b1-44b1-89d2-a0d63861a485",
   "metadata": {},
   "source": [
    "## Setting up the test parquet file as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fb156c4-1b40-47d9-a065-3ea2edb60a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"file:///home/iceberg/workspace/downloaded-data/yellow_tripdata_2024-01.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b2fde4-1e24-4d61-a684-f1d141c6804e",
   "metadata": {},
   "source": [
    "Now we check the data to get an idea of the size, structure and the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02404d7c-d0f3-40e3-8566-309b21876cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2964624\n",
      "Schema:\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n",
      "Data:\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-01-01 00:57:55|  2024-01-01 01:17:43|              1|         1.72|         1|                 N|         186|          79|           2|       17.7|  1.0|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              1|          1.8|         1|                 N|         140|         236|           1|       10.0|  3.5|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:17:06|  2024-01-01 00:35:01|              1|          4.7|         1|                 N|         236|          79|           1|       23.3|  3.5|    0.5|       3.0|         0.0|                  1.0|        31.3|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              1|          1.4|         1|                 N|          79|         211|           1|       10.0|  3.5|    0.5|       2.0|         0.0|                  1.0|        17.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              1|          0.8|         1|                 N|         211|         148|           1|        7.9|  3.5|    0.5|       3.2|         0.0|                  1.0|        16.1|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {df.count()}\")\n",
    "print(\"Schema:\")\n",
    "df.printSchema()\n",
    "print(\"Data:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8092ce-a336-48b4-9874-969ae3d8b0f3",
   "metadata": {},
   "source": [
    "## Creating Iceberg namespace under the catalog\n",
    "We create the namespace (schema) under the iceberg catalog `spark_iceberg_hive` we created in the Spark Session configs, and assign the s3 (minio) location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9117aa16-1313-42ae-92e1-5a1dbb2c595f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create namespace spark_iceberg_hive.spark_iceberg_hive LOCATION 's3://warehouse/spark-iceberg-hive/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358475ce-47c2-46e3-9b04-3c72671d092e",
   "metadata": {},
   "source": [
    "## Writing the data to Iceberg Table\n",
    "Finally, writing the data to the Iceberg table and timing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "920e67d3-8e4d-4f25-9d11-b463387f3dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.95098574800022\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "cc_main_df.writeTo(\"spark_iceberg_hive.spark_iceberg_hive.cc_main_2024_26_part_00000\").create()\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
